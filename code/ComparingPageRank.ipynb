{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import operator\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_text_file(filepath,word):\n",
    "     with open(filepath) as f:\n",
    "        a = f.readlines()\n",
    "        count = Counter(word for line in a)\n",
    "        return count[word]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_text_file('plaintext_articles/Zambia.txt','Zambia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_through_files(word):    \n",
    "    pathlist = Path('plaintext_articles')\n",
    "    Count_doc = dict()\n",
    "    for path in pathlist.iterdir():\n",
    "        path_in_str = str(path)\n",
    "        count = read_text_file(path_in_str,word)\n",
    "        Count_doc[path_in_str] = count\n",
    "    #print(Count_doc)\n",
    "    max_word = heapq.nlargest(100, Count_doc, key=Count_doc.get)\n",
    "    #max_word = sorted(Count_doc, key = Count_doc.get, reverse = True)[:100]\n",
    "    #max_word = max(Count_doc, key=Count_doc.get)\n",
    "    print(max_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plaintext_articles/Extinct_birds.txt', 'plaintext_articles/World_War_II.txt', 'plaintext_articles/Byzantine_Empire.txt', 'plaintext_articles/The_Holocaust.txt', 'plaintext_articles/Indo-Greek_Kingdom.txt', 'plaintext_articles/Leon_Trotsky.txt', 'plaintext_articles/List_of_lakes.txt', 'plaintext_articles/Japanese_grammar.txt', 'plaintext_articles/Vietnam_War.txt', 'plaintext_articles/List_of_sovereign_states.txt', 'plaintext_articles/Peru.txt', 'plaintext_articles/Guqin.txt', 'plaintext_articles/Elvis_Presley.txt', 'plaintext_articles/Condoleezza_Rice.txt', 'plaintext_articles/Sanskrit.txt', 'plaintext_articles/Argentina.txt', 'plaintext_articles/World_War_I.txt', 'plaintext_articles/Ronald_Reagan.txt', 'plaintext_articles/American_Civil_War.txt', 'plaintext_articles/Swastika.txt', 'plaintext_articles/La_Grande_Arm%C3%A9e.txt', 'plaintext_articles/The_Cantos.txt', 'plaintext_articles/Joseph_Stalin.txt', 'plaintext_articles/Muhammad.txt', 'plaintext_articles/Judaism.txt', 'plaintext_articles/Roman_Empire.txt', 'plaintext_articles/Battle_of_Normandy.txt', 'plaintext_articles/Floppy_disk.txt', 'plaintext_articles/Germany.txt', 'plaintext_articles/Paul_McCartney.txt', 'plaintext_articles/Dresden.txt', 'plaintext_articles/Eastern_Orthodox_Church.txt', 'plaintext_articles/Iraq_War.txt', 'plaintext_articles/History_of_Russia.txt', 'plaintext_articles/Manchester.txt', 'plaintext_articles/Paris.txt', 'plaintext_articles/Monopoly_%28game%29.txt', 'plaintext_articles/American_Revolutionary_War.txt', 'plaintext_articles/Islam.txt', 'plaintext_articles/Gottfried_Leibniz.txt', 'plaintext_articles/Tropical_cyclone.txt', 'plaintext_articles/Israel.txt', 'plaintext_articles/Magna_Carta.txt', 'plaintext_articles/Liberalism.txt', 'plaintext_articles/Cat.txt', 'plaintext_articles/Adolf_Hitler.txt', 'plaintext_articles/Spain.txt', 'plaintext_articles/Sassanid_Empire.txt', 'plaintext_articles/Theodore_Roosevelt.txt', 'plaintext_articles/Winston_Churchill.txt', 'plaintext_articles/United_States.txt', 'plaintext_articles/Roman_Catholic_Church.txt', 'plaintext_articles/Ottoman_Empire.txt', 'plaintext_articles/History_of_the_Royal_Australian_Navy.txt', 'plaintext_articles/Schizophrenia.txt', 'plaintext_articles/Jet_engine.txt', 'plaintext_articles/Bretton_Woods_system.txt', 'plaintext_articles/Tony_Blair.txt', 'plaintext_articles/England.txt', 'plaintext_articles/George_W._Bush.txt', 'plaintext_articles/History_of_Poland_%281945%E2%80%931989%29.txt', 'plaintext_articles/Glasgow.txt', 'plaintext_articles/Birmingham.txt', 'plaintext_articles/Hinduism.txt', 'plaintext_articles/History_of_South_Africa.txt', 'plaintext_articles/Pope_John_Paul_II.txt', 'plaintext_articles/NATO.txt', 'plaintext_articles/Race.txt', 'plaintext_articles/History_of_painting.txt', 'plaintext_articles/Genghis_Khan.txt', 'plaintext_articles/Nagorno-Karabakh_War.txt', 'plaintext_articles/Driving_on_the_left_or_right.txt', 'plaintext_articles/Evolution.txt', 'plaintext_articles/Finland.txt', 'plaintext_articles/RMS_Titanic.txt', 'plaintext_articles/Trinity.txt', 'plaintext_articles/Albert_Einstein.txt', 'plaintext_articles/United_Kingdom.txt', 'plaintext_articles/Elizabeth_II_of_the_United_Kingdom.txt', 'plaintext_articles/Education_in_the_United_States.txt', 'plaintext_articles/Spanish_Inquisition.txt', 'plaintext_articles/Trench_warfare.txt', 'plaintext_articles/Nepenthes_rajah.txt', 'plaintext_articles/Shroud_of_Turin.txt', 'plaintext_articles/Ancient_Rome.txt', 'plaintext_articles/Economy_of_Pakistan.txt', 'plaintext_articles/Tiger_Woods.txt', 'plaintext_articles/International_Red_Cross_and_Red_Crescent_Movement.txt', 'plaintext_articles/Law.txt', 'plaintext_articles/Music_of_the_United_States.txt', 'plaintext_articles/Slavery.txt', 'plaintext_articles/Buddhism.txt', 'plaintext_articles/Philosophy.txt', 'plaintext_articles/Beijing.txt', 'plaintext_articles/Iraq_and_weapons_of_mass_destruction.txt', 'plaintext_articles/Baseball.txt', 'plaintext_articles/Louis_XIV_of_France.txt', 'plaintext_articles/Diamond.txt', 'plaintext_articles/Zionism.txt', 'plaintext_articles/British_monarchy.txt']\n"
     ]
    }
   ],
   "source": [
    "run_through_files('Zambia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_p_matrix(matrix):\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    return matrix / row_sums[:]\n",
    "\n",
    "def fix_dangling_node(H):\n",
    "    # Finds which rows have all elements zeros\n",
    "    # d is a column vector that identifies dangling nodes\n",
    "    d = ~(H.any(axis=1))\n",
    "    # w is a uniform row vector\n",
    "    w = np.full(H.shape[1], 1/H.shape[1])\n",
    "    S = H + d*w\n",
    "    return S\n",
    "\n",
    "def form_google_matrix(S, v=None, alpha=0.85):\n",
    "    if v is None:\n",
    "         v = np.full(S.shape[1], 1/S.shape[1])\n",
    "    one_vector = np.ones(S.shape[0])\n",
    "    return alpha*S + (1-alpha)*one_vector*v\n",
    "\n",
    "\n",
    "def power_method2(H, v=None, alpha=0.85):\n",
    "    if v is None:\n",
    "         v = np.full(H.shape[1], 1/H.shape[1])\n",
    "    d = ~(H.any(axis=1))\n",
    "    w = np.full(H.shape[1], 1/H.shape[1])\n",
    "    # start with pi = v\n",
    "    pi = np.zeros(H.shape[0])\n",
    "    pi_new = v\n",
    "    cnt = 0\n",
    "    print(H.shape)\n",
    "    while not np.allclose(pi_new, pi):\n",
    "        pi = pi_new\n",
    "        pi_new = alpha * pi * H + alpha * (pi*d) * w + (1-alpha) * v\n",
    "        cnt += 1\n",
    "    #print(\"Power method2 went through {} iteration\".format(cnt))\n",
    "    return pi_new\n",
    "\n",
    "\n",
    "def power_method1(H, v=None, alpha=0.85):\n",
    "    S = fix_dangling_node(H)\n",
    "    G = form_google_matrix(S, v=None, alpha=alpha)\n",
    "\n",
    "    pi_new = np.full(G.shape[1], 1/G.shape[1])\n",
    "    pi = np.zeros(G.shape[0])\n",
    "    cnt = 0\n",
    "    while not np.allclose(pi_new, pi):\n",
    "        pi = pi_new\n",
    "        pi_new = pi * G\n",
    "        cnt += 1\n",
    "    #print(\"Power method1 went through {} iteration\".format(cnt))\n",
    "    return pi_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_plots(g):\n",
    "    def make_p_matrix(matrix):\n",
    "        row_sums = matrix.sum(axis=1)\n",
    "        return matrix / row_sums[:]\n",
    "    \n",
    "    def power_method2(H, v=None, alpha=0.85):\n",
    "        if v is None:\n",
    "             v = np.full(H.shape[1], 1/H.shape[1])\n",
    "        d = ~(H.any(axis=1))\n",
    "        w = np.full(H.shape[1], 1/H.shape[1])\n",
    "        # start with pi = v\n",
    "        pi = np.zeros(H.shape[0])\n",
    "        pi_new = v\n",
    "        cnt = 0\n",
    "        while not np.allclose(pi_new, pi):\n",
    "            pi = pi_new\n",
    "            pi_new = alpha * pi * H + alpha * (pi*d) * w + (1-alpha) * v\n",
    "            cnt += 1\n",
    "        #print(\"Power method2 went through {} iteration\".format(cnt))\n",
    "        return pi_new\n",
    "    x = []\n",
    "    n = len(g)\n",
    "    m = make_p_matrix(np.matrix(networkx.convert_matrix.to_numpy_matrix(g), dtype=np.float64))\n",
    "    c = power_method2(m)\n",
    "    for i in range(n):\n",
    "        x.append(c.item(i))  # PageRank Probability\n",
    "    \n",
    "    #brandes = Brandes(g)\n",
    "    \"\"\" brandes = nx.betweenness_centrality(g)\n",
    "    x = []\n",
    "    y = []\n",
    "    n = len(g)\n",
    "    for i in range(n):\n",
    "        x.append(c.item(i))  # PageRank Probability\n",
    "    for node in g:\n",
    "        y.append(brandes[node]) # Betweenness\n",
    "    plt.plot(y, x, \"o\")\n",
    "    plt.xlabel('betweenness')\n",
    "    plt.ylabel('probability output of PageRank')\n",
    "    plt.show()\"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_graph_from_dataset(filepath):\n",
    "    with open(filepath) as f:\n",
    "        a = f.readlines()\n",
    "        edges = []\n",
    "        for line in a:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            else:\n",
    "                a,b = line.strip().split()\n",
    "                edges.append((a,b))\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = make_graph_from_dataset('links.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
